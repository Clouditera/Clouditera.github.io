---
layout:     post
title:      "第50期|GPTSecurity周报"
date:       2024-08-21 16:00:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - Security
    - AIGC
    - GPTSecurity周报
---


![这是一张图片](https://www.gptsecurity.info/img/in-post/0807/01.jpg)

GPTSecurity是一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练Transformer（GPT）、人工智能生成内容（AIGC）以及大语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。现为了更好地知悉近一周的贡献内容，现总结如下。


## Security Papers



#### 针对硬件安全的进化型大语言模型：一项比较性调查研究

简介：在芯片制造前自动化检测和缓解硬件安全漏洞至关重要，因为后期修复成本高昂且不现实。现代硬件的复杂性也增加了未知漏洞的风险。大语言模型（LLM）在半导体领域中，有潜力自动纠正设计中的安全漏洞。本研究聚焦于LLM在寄存器传输级设计中的应用，评估其独立解决安全漏洞的能力，并探讨了方法论、可扩展性、可解释性，以及未来的研究方向，旨在通过特定领域知识提升模型性能，实现硬件安全的自动化测量和风险缓解。

*链接：https://arxiv.org/pdf/2404.16651*

#### 大语言模型中的机器遗忘

简介：大语言模型（LLMs）因能自动生成智能内容而备受瞩目，但也面临安全和隐私挑战。为应对这些问题，本文提出了一个机器遗忘框架，旨在防止LLMs生成有害、幻觉或侵犯隐私的响应，同时保留其标准输出功能。通过评估模型识别需遗忘的对话，并利用距离损失和簇均值正损失引导模型输出向更优结果，而不损害其推理和性能。实验证明，该方法能有效实现遗忘目标，且对模型性能影响不大。

*链接：https://arxiv.org/pdf/2404.16841*

#### 用于大语言模型（LLMs）的快速自适应对抗性提示

简介：大语言模型（LLMs）虽取得显著进展，但易受越狱攻击影响，产生不当内容。传统手动寻找对抗性提示方法效率低下。自动生成对抗性提示易被检测，且扩展性差。本文提出AdvPrompter，一种新型LLM，能在几秒内生成易读的对抗性提示，速度提升约800倍。AdvPrompter采用无需目标LLM梯度信息的新算法训练，通过优化预测和微调两个步骤生成后缀，使目标LLM在不改变输入指令含义的情况下生成有害响应。实验显示，AdvPrompter在AdvBench数据集上达到最佳效果，并可迁移至闭源LLM API。此外，通过在AdvPrompter生成的合成数据集上微调，LLMs可增强对越狱攻击的鲁棒性，同时保持高性能。

*链接：https://arxiv.org/pdf/2404.16873*

#### 针对大语言模型第三方API的攻击

简介：大语言模型（LLM）服务通过插件生态系统与第三方API服务交互，虽然增强了功能，但也带来了安全风险。本文提出了一个新框架，专门用于检测整合第三方服务的LLM平台的安全漏洞。研究团队在多个领域识别出真实世界的恶意攻击，这些攻击能够悄无声息地改变LLM的输出结果。文章讨论了第三方API集成所面临的挑战，并提出了加强LLM生态系统安全性和安全性的战略性建议。相关代码已在指定网址发布。

*链接：https://arxiv.org/pdf/2404.16891*

#### 在由大语言模型（LLM）支持的应用程序中的人类不可感知的检索投毒攻击

简介：当前，借助先进的大语言模型（LLM）应用开发框架，应用程序能轻松利用检索增强生成（RAG）技术扩展LLM知识库。但这些框架未充分考虑外部内容风险，易受攻击者破坏。本文揭示了一种名为“检索投毒”的新威胁，攻击者可通过RAG过程诱导应用产生恶意回应。攻击者分析框架后，制作出看似无害却能误导RAG参考源的文档，导致应用生成错误响应。初步实验显示，攻击者能以88.33%的成功率误导LLM，现实应用中成功率达66.67%，突显了检索投毒的严重性。

*链接：https://arxiv.org/pdf/2404.17196*

#### FormAI-v2 数据集：标记由大语言模型生成的代码中的漏洞

简介：本研究对当前先进的大语言模型（LLMs）进行了比较分析，探讨了它们在无特定指令下编写简单C程序时产生安全漏洞的倾向。先前研究中对这些模型生成的代码安全性缺乏深入探讨，本研究填补了这一空白。基于PROMISE '23上介绍的FormAI数据集，本研究扩展出了FormAI-v2，包含265,000个由不同LLMs生成的C程序，并通过高效SMT模型检查器（ESBMC）进行形式验证，标记了源代码中的漏洞。研究发现，至少63.47%的程序存在安全漏洞，不同模型间的差异不大。研究结果表明，尽管LLMs在代码生成上展现出巨大潜力，但在生产环境中应用其生成的代码前，必须进行风险评估和验证。

*链接：https://arxiv.org/pdf/2404.18353*




![secgeek-foot](https://www.gptsecurity.info/img/secgeek-foot.png)
