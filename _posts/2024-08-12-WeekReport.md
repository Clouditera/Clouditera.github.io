---
layout:     post
title:      "第58期|GPTSecurity周报"
date:       2024-08-12 17:00:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - Security
    - AIGC
    - GPTSecurity周报
---

![这是一张图片](https://www.gptsecurity.info/img/in-post/0807/01.jpg)


GPTSecurity是一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练Transformer（GPT）、人工智能生成内容（AIGC）以及大语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。现为了更好地知悉近一周的贡献内容，现总结如下。


## Security Papers




   
- **你的AI生成的代码真的安全吗？通过CodeSecEval评估大语言模型在安全代码生成方面的表现**

  
简介：大语言模型（LLMs）在代码生成和修复方面取得显著进展，但其训练数据源自未经过滤的开源代码库（如GitHub），存在传播安全漏洞的风险。尽管有研究关注代码LLM的安全性，但对其安全特性的全面评估尚不足。为此，研究者提出了CodeSecEval，一个包含44种关键漏洞类型和180个样本的数据集，用于自动评估代码模型在生成和修复代码中的安全表现。实验发现现有模型常忽视安全问题，导致生成易受攻击的代码。为解决这一问题，研究者提出了利用漏洞感知信息和不安全代码解释的策略。研究进一步强调了某些漏洞类型对模型性能的特别挑战，期望这项工作能促进软件工程社区改进LLM的训练和应用方法，实现更安全可靠的模型部署。

  *链接：https://arxiv.org/pdf/2407.02395*



-  **SOS！针对开源大语言模型的软提示攻击**
  
  简介：开源大语言模型（LLMs）因其可定制性、微调性和自由使用性，深受公众和工业界欢迎。然而，一些开源的LLMs在使用前需要获得批准，促使第三方发布更易获取的版本，这些版本虽受用户青睐，却增加了训练时间攻击的风险。研究者提出了一种新的训练时间攻击SOS，其计算需求低，无需干净数据或修改模型权重，保持了模型的实用性完整。SOS攻击解决了后门、越狱和提示窃取等安全问题。实验结果表明，SOS攻击在所有评估目标上都表现出有效性。此外，研究者还提出了版权令牌技术，允许用户标记其受版权保护的内容，以防止模型使用这些内容。

   *链接：https://arxiv.org/pdf/2407.03160*



- **DART：用于大语言模型安全性的深度对抗自动红队测试**

简介：手动红队测试用于识别大语言模型（LLMs）的漏洞，但成本高且难以扩展。相比之下，自动红队测试利用红队LLM生成对抗性提示，提供了可扩展的安全漏洞检测方法。然而，目标LLM的安全漏洞动态变化，构建强大的自动红队LLM具有挑战性。为解决这一问题，研究者提出了DART框架，通过迭代方式使红队LLM和目标LLM深度动态交互。红队LLM根据目标LLM的响应和攻击多样性调整攻击方向，目标LLM通过主动学习数据选择机制增强安全性。实验结果显示，DART显著降低了目标LLM的安全风险。在Anthropic Harmless数据集上的评估中，DART将违规风险减少了53.4%。

   *链接：https://arxiv.org/pdf/2407.03876*



-  **保护多轮对话语言模型免受分布式后门触发器攻击**
  
简介：尽管多轮对话大型语言模型（LLMs）是最受欢迎的LLM应用之一，但其安全性研究却相对不足。LLMs容易受到数据污染后门攻击的影响，攻击者通过操控训练数据使模型在预设触发条件下输出恶意响应。在多轮对话中，LLMs面临更隐蔽和有害的后门攻击风险，后门触发器可能跨越多个对话环节，增加了上下文驱动攻击的潜在威胁。研究者探索了一种新型的分布式后门触发器攻击，作为对手工具箱的额外工具，并揭示其对现有防御策略的挑战。为应对这一问题，研究者提出了基于对比解码的新型防御方法，能够有效降低后门攻击的影响，且计算成本相对较低。

   *链接：https://arxiv.org/pdf/2407.04151*



-  **使用标记替换防御语法文本后门攻击**
  
简介：文本后门攻击对大语言模型（LLM）的安全性构成重大威胁。它在训练阶段向受害模型嵌入精心选择的触发器，导致模型误将包含这些触发器的输入预测为特定类别。先前的后门防御方法主要针对特殊标记的触发器，而对基于语法的触发器处理不足。为此，本文提出了一种新的在线防御算法，用完全不同的词替换句子中语义有意义的词，但保留句法模板或特殊标记，然后比较预测标签来判断是否存在触发器。实验结果显示，该算法有效对抗这两种类型的触发器，为保障模型完整性提供了全面的防御策略。

   *链接：https://arxiv.org/pdf/2407.04179*



-  **大语言模型的越狱攻击及防御：一项调查**
  
简介：大语言模型（LLMs）在多种文本生成任务中表现出色，但其过度辅助特性引发了“越狱”挑战，即通过对抗性提示设计诱使模型生成违反政策和社会伦理的恶意响应。随着利用LLMs不同漏洞的越狱攻击方法的出现，相应的安全对齐措施不断演进。本文提出了详尽的越狱攻击与防御方法分类体系，将攻击分为黑盒和白盒两类，防御则分为提示级和模型级两种。研究者进一步细分了这些方法的子类，并通过图示展示它们之间的关系。通过调查和比较当前的评估方法，研究者的研究旨在推动保护LLMs免受对抗攻击的未来研究和实际应用，提升对该领域的理解并促进更安全的LLMs开发。

   *链接：https://arxiv.org/pdf/2407.04295*

![secgeek-foot](https://www.gptsecurity.info/img/secgeek-foot.png)
