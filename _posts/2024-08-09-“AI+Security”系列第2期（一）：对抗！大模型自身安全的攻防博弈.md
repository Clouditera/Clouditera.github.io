---
layout:     post
title:      "“AI+Security”系列第2期（一）：对抗！大模型自身安全的攻防博弈"
date:       2024-08-09 15:00:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
tags:
    - Security
    - AIGC
    - “AI+Security”系列
  
---


![这是一张图片](https://www.gptsecurity.info/img/in-post/0807/01.jpg)

近日，由安全极客、Wisemodel 社区和 InForSec 网络安全研究国际学术论坛联合主办的“AI+Security”系列第 2 期——**对抗！大模型自身安全的攻防博弈**线上活动如期举行。本次活动邀请了**君同未来创始人兼 CEO 韩蒙、前阿里云高级安全专家郑瀚、ChaMd5 AI 组负责人宁宇飞、始智 AI wisemodel 创始人兼 CEO 刘道全、云起无垠创始人兼 CEO 沈凯文和金睛云华技术合伙人孙志敏**，他们针对大模型安全进行了精彩分享，并且围绕“大模型自身安全”展开了圆桌讨论，深入探讨了大模型安全现状、未来发展趋势以及前景市场等相关问题。

活动开始时，安全极客主理人王书辉对各位嘉宾和观众的到来表示欢迎，同时简要概述了本次活动的话题和探讨方向，并欢迎大家积极参与，共同分享“AI+Security”的相关内容，以促进相关技术进步，为产业贡献力量。

## 01 人工智能风险治理机遇与挑战


人工智能（AI）技术的迅速发展带来了许多机遇，同时也引发了一系列的风险和挑战。特别是大模型技术的广泛应用，如GPT-4、BERT等，虽然推动了AI能力的提升，但也暴露了数据隐私泄露、生成内容违规、模型偏见等诸多问题。因此，如何有效治理人工智能风险，确保其安全、可靠、公平地发展，成为当前社会亟待解决的重要课题。

![这是一张图片](https://www.gptsecurity.info/img/in-post/0809/05.jpg)

君同未来创始人兼 CEO 韩蒙博士指出，当前大语言模型所面临的风险类型涵盖内生安全、伴生安全、应用安全问题。其中内生安全问题包含非对抗性风险和对抗性风险。非对抗性风险有模型幻觉、数据偏见、机密数据泄露等；对抗性风险包括模型对抗攻击、模型越狱攻击、模型目标劫持攻击、模型提示词攻击等。

![这是一张图片](https://www.gptsecurity.info/img/in-post/0809/06.jpg)

基于此，韩蒙博士表示，君同未来深入探索了安全风险识别能力、攻击意图识别能力、正向回答生成能力、输出内容改写能力，并研发了大模型安全评测平台、大模型风险监控平台和大模型安全对齐平台，以提升大模型的安全性和可靠性。

## 02 面向LLM的漏洞挖掘与对齐防御研究


随着人工智能技术的迅猛发展，大语言模型（LLM）凭借其卓越的多任务学习能力和泛化能力，在各行各业中发挥着越来越重要的作用。这些模型能够处理庞大的参数量，深度整合企业内部数据，为实际业务场景提供精准服务。然而，随着云端和端侧LLM的互补发展以及开源LLM的广泛普及，虽然为小型开发者带来了前所未有的便利和效率提升，但同时也带来了新的安全风险和挑战。

![这是一张图片](https://www.gptsecurity.info/img/in-post/0809/07.jpg)

LLM越狱攻击作为一种典型的安全威胁，对LLM的安全稳定性构成了重大影响。郑瀚先生指出，与传统网络安全攻击技术相比，LLM安全攻击已经进入了一个不确定的概率范式。这一范式转移带来的最大挑战是漏洞搜索空间的无限扩大。由于大型模型本质上是由数千亿参数构成的复杂概率模型，它们的行为和输出具有高度的不确定性和复杂性，这使得传统的基于形式逻辑的漏洞挖掘和防御方法变得不再适用。

越狱攻击与LLM的对齐方法紧密相关，其核心目标在于破坏模型开发人员所施加的基于人类价值观的约束和其他限制，迫使模型在面对恶意问题时提供正确的答案，而不是选择拒绝回答。这种攻击方式不仅对模型的安全性构成威胁，更对模型的可靠性和可控性提出了严峻挑战。

## 03 AI/机器学习供应链攻击


在网络安全领域，公共仓库中的组件和库极易遭受恶意用户的攻击。这些恶意用户通过进行“名字抢注”或“拼写抢注”，即注册与知名实体相似的名称，来利用其信誉传送恶意负载。这种攻击策略在Python软件包索引（PyPI）和Node软件包管理器（npm）等仓库中已屡有发生，而且人工智能和机器学习（AI/ML）供应链也正在成为此类攻击的目标。据Sophos的报告显示，微软的域名抢注率达61%，Twitter为74%，Facebook为81%，谷歌为83%，苹果则为86%。

![这是一张图片](https://www.gptsecurity.info/img/in-post/0809/08.jpg)
 
宁宇飞先生指出，在公共ML仓库（如Hugging Face）中，恶意用户能够上传被破坏的模型，并将其伪装成来自可信来源的发布工件。尽管Hugging Face设有验证流程，但存在容易被忽略验证标志等问题。此外，还发现了多个冒用知名公司名称的仓库，比如存在导致API密钥泄露的.pth文件以及假冒llama的仓库等情况。

## 04 大模型自身安全

在圆桌讨论中，君同未来创始人兼CEO韩蒙、前阿里云高级安全专家郑瀚、ChaMd5 AI组负责人宁宇飞、始智AI wisemodel创始人兼CEO刘道全、云起无垠创始人兼CEO沈凯文、金睛云华技术合伙人孙志敏等共同参与，深入探讨了大模型自身安全相关问题，具体包括以下几个方面：

1. 随着大模型在各领域的广泛应用，探讨未来几年大模型安全性所面临的最大挑战，以及科研与产业界应采取的关键措施来应对这些挑战？

2. 在信息安全/网络安全领域中，网络攻击以往通常由专业人才实施。当下大模型不断发展，如今所探讨的大模型安全，是否会降低网络攻击的门槛，进而使普通人也能够进行网络攻击？其产生的影响是否会更大？

3. 鉴于各行各业在利用大模型技术突破传统瓶颈时，有的使用开源模型，有的使用闭源商业模型，而社会各界对开源模型和闭源模型的安全性存在不同观点，有人认为开源模型更安全，也有专家认为闭源模型更安全。针对这一问题，询问各位专家的看法，并探讨企业在使用开源模型和闭源模型时是否有不同的安全关注点？

4. 鉴于目前大模型应用蓬勃发展，围绕大模型安全的市场规模、爆发节点预测及重点商业化方向，讨论未来大模型安全的商业化市场前景？

5. 当下大模型安全是备受关注的重点，那么，大模型安全是给大模型公司创造了机会和需求，还是为广大创新创业公司创造了机会？

6. 鉴于之前Open AI组建了Super alignment超级对齐团队，以及Open AI的首席科学家离职并开启了自己的新项目——安全超级智能，这些都充分体现了安全性在大模型领域的重要性。站在人类命运共同体的角度，探讨如何保证大模型自身的安全可控，防止被未来的超级智能力量所颠覆？

## 写在最后

本次分享活动可谓是干货十足，对大模型自身安全相关问题进行了深入的探讨。关于本次活动嘉宾的精彩分享的系列内容，我们会逐一进行整理，并陆续发布，敬请大家期待！

此外，“AI + Security”系列的第三期专题分享活动将于9月初左右与大家在线下见面。届时，我们将邀请来自人工智能（AI）和网络安全领域的行业专家以及领军人物共同参与分享，深入探讨并分享关于“AI + Security”技术理念的独到见解和丰富经验。

欢迎大家关注“安全极客”，我们热切期待您的加入，一同推动AI与安全技术的融合与创新，共创美好未来！




