---
layout:     post
title:      "【论文速读】| PROMPTFUZZ：利用模糊测试技术对大语言模型中的提示注入进行鲁棒性测试"
date:       2024-10-16 10:30:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
tags:
    - Security
    - AIGC
    - 论文速读
---


![这是一张图片](https://www.gptsecurity.info/img/in-post/0807/01.jpg)

## 基本信息

**原文标题**: PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs

**原文作者**: Jiahao Yu, Yangguang Shao, Hanwen Miao, Junzheng Shi, Xinyu Xing

**作者单位**: Northwestern University, Institute of Information Engineering, Chinese Academy of Sciences, University of the Chinese Academy of Sciences

**关键词**: 大语言模型（LLMs），提示注入攻击，模糊测试，鲁棒性测试，自动化测试

**原文链接**: https://arxiv.org/pdf/2409.14729

**开源代码**:https://github.com/sherdencooper/PromptFuzz

## 论文要点

**论文简介**:本文提出了PROMPTFUZZ，一个创新框架，用于测试大语言模型（LLMs）在提示注入攻击下的鲁棒性。提示注入攻击通过覆盖原始指令，篡改生成的内容，严重威胁了LLMs的应用安全。PROMPTFUZZ借鉴了软件模糊测试的思路，自动生成多样化的提示注入，并系统评估模型在各种攻击场景下的表现。


![这是一张图片](https://www.gptsecurity.info/img/in-post/1016/01.png)

PROMPTFUZZ的测试过程分为准备阶段和聚焦阶段。准备阶段从初始种子提示生成大量变异提示并测试其在防御机制下的效果；聚焦阶段进一步优化生成过程，集中资源生成更具攻击性的提示，以揭示LLMs的潜在漏洞。实验表明，PROMPTFUZZ在提示注入攻击竞赛中取得第7名，并成功发现防御机制强大模型中的漏洞，显示了其在提升LLMs安全性中的重要作用。

**研究目的**:本研究的主要目的是提升LLMs在提示注入攻击下的安全性。随着LLMs在各类自然语言处理任务中的应用扩大，其面临的安全挑战也日益增多，尤其是提示注入攻击可能篡改模型指令，导致错误或有害输出。现有的手工测试方法覆盖面有限，耗时较长，因此需要自动化测试框架来系统化评估模型的鲁棒性。PROMPTFUZZ通过自动化、全面的测试方法，旨在提高LLMs在应对提示注入攻击时的安全性。

## 引言

随着LLMs在如在线助手、广告审核、代码补全等任务中的应用愈加广泛，安全性问题也愈发重要。其中，提示注入攻击通过恶意提示篡改模型输出，是当前最严重的威胁之一。传统的手工设计测试耗时且难以覆盖全部场景，因此自动化测试工具显得尤为重要。为此，PROMPTFUZZ框架通过生成多样化的提示注入来自动化测试LLMs的安全性，评估其防御能力。

PROMPTFUZZ利用模糊测试的技术框架，分为准备阶段和聚焦阶段。准备阶段生成并测试初始提示，聚焦阶段进一步优化生成过程，以便发现模型的潜在弱点。实验中，PROMPTFUZZ展示了其效率，超越了大多数人工测试者，为LLMs的安全评估提供了新的思路。

## 研究背景

**大语言模型**：大语言模型（LLMs）基于深度学习，能够理解和生成类人文本。其核心为自注意力机制，使得模型可以处理长距离依赖，广泛应用于自然语言处理任务，如文本生成、翻译和对话等。近年来，OpenAI的GPT系列和Meta的LLaMA等模型得到了广泛关注。然而，随着这些模型的应用日益增加，安全性问题也逐渐暴露，尤其是提示注入攻击。因此，研究如何提升LLMs在安全威胁下的鲁棒性变得至关重要。

![这是一张图片](https://www.gptsecurity.info/img/in-post/1016/02.png)

**模糊测试**：模糊测试是一种自动化测试方法，通过随机输入或变异数据来揭示系统中的漏洞，广泛用于检测缓冲区溢出、内存泄露等安全问题。在LLMs的安全评估中，模糊测试的思路被用于生成多样化的变异提示，从而评估模型在提示注入攻击下的表现。通过大量测试，模糊测试能够揭示模型的安全弱点，帮助提升其鲁棒性和防御能力。

## 相关工作

提示注入攻击作为LLMs面临的主要安全威胁之一，近年来引发了广泛关注。现有的测试大多依赖手工设计的“红队”方法，虽然有效但覆盖有限，且耗时较长。自动化攻击生成工具如GPTFuzz和GCG-injection等尝试通过生成攻击提示来提升测试效率，但大多集中在特定场景，缺乏多样性。PROMPTFUZZ的设计弥补了这一空白，借鉴模糊测试的思路，提供了一个自动化框架，能够生成多样化的注入提示，有效评估LLMs的鲁棒性。

## 实验设计

**PROMPTFUZZ概述**：PROMPTFUZZ框架结合了模糊测试技术，旨在评估LLMs在提示注入攻击下的鲁棒性。其设计分为准备阶段和聚焦阶段。准备阶段通过变异初始种子提示生成大量注入提示，并在目标模型上进行初步测试。聚焦阶段则进一步优化生成高效的提示注入，最大限度发现模型中的漏洞。实验结果表明，PROMPTFUZZ不仅在竞赛中表现优异，还能有效绕过防御机制，揭示LLMs中的隐蔽漏洞。

![这是一张图片](https://www.gptsecurity.info/img/in-post/1016/03.png)

**准备阶段**：PROMPTFUZZ在准备阶段收集了所有手工编写的种子提示，并通过多种变异方法（如扩展、缩短等）生成大量变异提示。这些变异提示会在LLMs上进行测试，分析其绕过防御机制的效果。最终，表现最好的变异提示将被保留，以供聚焦阶段进一步优化。

**聚焦阶段**：聚焦阶段旨在集中资源于最有潜力的种子提示，并对这些种子进行更深层次的变异优化。PROMPTFUZZ通过分析准备阶段的结果，选择成功率较高的种子进行进一步变异，生成更具针对性和攻击性的提示注入。这一阶段会持续迭代，直到满足终止条件（例如达到最大迭代次数或时间限制）。

## 研究实验

**实验设置**：为了验证PROMPTFUZZ的有效性，研究团队在提示注入攻击竞赛中进行了测试。PROMPTFUZZ在短时间内便取得了第7名的成绩，超越了4000多名参赛者。实验还表明，PROMPTFUZZ生成的提示能够绕过多种防御机制，成功发现漏洞，证明其在真实世界应用中的有效性。

![这是一张图片](https://www.gptsecurity.info/img/in-post/1016/04.png)

**实验结果**：PROMPTFUZZ生成的提示在提示注入攻击中表现优异，成功率显著高于手工设计和其他自动化方法。无论在比赛还是真实应用中，PROMPTFUZZ均展示出高效的漏洞发现能力，即使在微调后的LLMs上，仍能生成有效攻击提示。

![这是一张图片](https://www.gptsecurity.info/img/in-post/1016/05.png)

## 研究评估

PROMPTFUZZ还被用于构建微调数据集，提升LLMs对提示注入攻击的防御能力。实验表明，尽管经过微调的模型在一定程度上提高了安全性，但PROMPTFUZZ依然能够生成有效的攻击提示。这表明，对LLMs的持续测试和防御强化是必要的。

![这是一张图片](https://www.gptsecurity.info/img/in-post/1016/06.png)

## 论文结论

PROMPTFUZZ为LLMs的鲁棒性测试提供了一个高效、全面的自动化框架，能够系统评估模型在提示注入攻击下的安全性。通过生成多样化的提示注入，PROMPTFUZZ在强大防御机制下依然能够揭示潜在漏洞。实际应用证明，PROMPTFUZZ在竞赛和真实世界中表现出色，有效提高了LLMs在提示注入攻击下的安全性。研究团队通过开源代码，希望进一步推动LLMs安全性研究，帮助开发者提高模型的鲁棒性。

原作者：论文解读智能体

校对：小椰风

![这是一张图片](https://www.gptsecurity.info/img/in-post/0813/08.webp)







