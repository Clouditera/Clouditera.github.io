---
layout:     post
title:      "第68期|GPTSecurity周报"
date:       2024-09-25 16:00:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - Security
    - AIGC
    - GPTSecurity周报
---


![这是一张图片](https://www.gptsecurity.info/img/in-post/0807/01.jpg)

GPTSecurity是一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练Transformer（GPT）、人工智能生成内容（AIGC）以及大语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。现为了更好地知悉近一周的贡献内容，现总结如下。

#### 代码漏洞检测：新兴大语言模型的比较分析

简介：由于对开源项目的高度依赖，软件开发中漏洞问题日益增长的趋势最近受到了相当大的关注。研究者对大语言模型（LLMs）在识别代码库中的漏洞方面的有效性进行了研究，重点关注 LLM 技术的最新进展。通过比较分析，研究者评估了新兴的大语言模型，即 Llama、CodeLlama、Gemma 和 CodeGemma 的性能，并与已有的先进模型如 BERT、RoBERTa 和 GPT-3 进行了对比。研究者的研究旨在揭示大语言模型在漏洞检测方面的能力，为提高不同开源存储库中的软件安全实践做出贡献。研究者观察到，在用于检测软件安全漏洞的大语言模型的最新成员中，CodeGemma 实现了最高的 F1 分数 58 和召回率 87。

*链接：https://arxiv.org/abs/2409.10490*

#### 基于大语言模型的代码补全工具的安全攻击

简介：大语言模型的快速发展催生了新一代基于大语言模型的代码补全工具（LCCTs）。研究者指出，LCCTs 有独特的工作流程，将多个信息源作为输入，并优先考虑代码建议而非自然语言交互，这引入了独特的安全挑战，且常依赖专有代码数据集训练，易致敏感数据泄露。研究者针对越狱和训练数据提取攻击这两种安全风险，开发攻击方法。实验结果显示，对 GitHub Copilot 越狱攻击成功率达 99.4%，对 Amazon Q 为 46.3%，还从 GitHub Copilot 提取出敏感用户数据。研究表明，基于代码的攻击方法对通用大语言模型也有效，凸显安全问题。这些发现强调了 LCCTs面临的安全挑战，也为加强其安全框架提供方向。

*链接：https://arxiv.org/abs/2408.11006*

#### 大语言模型作为端到端的安全代码生成器的效果如何？

简介：大语言模型（如 GPT-4）的快速发展改变了软件工程格局。研究者对大语言模型作为端到端安全代码生成器的效果展开研究。他们研究了 GPT-3.5 和 GPT-4 识别和修复包括自身在内的四种流行大语言模型生成代码中漏洞的能力。通过审查 4900 段代码，发现大语言模型缺乏对场景相关安全风险的认识，生成超 75%有漏洞代码，且无法准确识别自身生成代码中的漏洞。在修复其他模型生成的不安全代码时成功率为 33.2%至 59.6%，但修复自身代码表现不佳。为解决单次修复局限性，研究者开发轻量级工具，借助语义分析引擎，将修复成功率提高到 65.9%至 85.5%。该研究为提升大语言模型生成安全代码的能力提供了方向。

*链接：https://arxiv.org/abs/2408.10495*

#### CodeMirage：大语言模型生成代码中的幻觉

简介：大语言模型在程序生成和无代码自动化方面展现出巨大潜力，但容易产生幻觉。研究者指出，虽然文本生成中大语言模型幻觉的研究很多，但代码生成中也存在类似现象，如生成的代码可能有语法、逻辑错误及安全漏洞等问题。鉴于其广泛应用，研究代码生成中的幻觉势在必行。研究者首次尝试研究此问题，引入代码幻觉定义和分类法，提出基准数据集 CodeMirage，包含 GPT-3.5 为 Python 编程问题生成的幻觉代码片段。他们还提出检测方法，对 CodeLLaMA、GPT-3.5 和 GPT-4 等模型进行实验，发现 GPT-4 在 HumanEval 数据集上表现最佳，在 MBPP 数据集上与微调后的 CodeBERT 基线相当。最后，研究者讨论了缓解代码幻觉的策略并总结了工作。

*链接：https://arxiv.org/abs/2408.08333*

#### 用于高效入侵检测系统的 Transformer 和大语言模型：全面综述

简介：随着 Transformer 大语言模型取得重大进步，自然语言处理在文本生成和用户交互方面能力增强，拓展至众多领域，其中网络安全领域受益颇丰。网络安全中许多需保护和交换的参数以文本和表格数据形式存在，使自然语言处理成为增强通信协议安全措施的有力工具。本综述全面分析 Transformer 和大语言模型在网络威胁检测系统中的应用。介绍了论文选择方法和文献计量分析框架，讨论了 Transformer 基础，包括网络攻击背景信息和常用数据集。探索其在入侵检测系统中的应用，涵盖多种架构，如基于注意力的模型、BERT 和 GPT 等大语言模型、CNN/LSTM-Transformer 混合模型及新兴的 ViTs 等。还探讨了其在不同环境和应用中的实现，包括计算机网络、物联网等。同时指出研究挑战和未来方向，如可解释性等问题。最后总结成果，强调其重要性并提出进一步研究方向。

*链接：https://arxiv.org/abs/2408.07583*


#### 用于安全代码评估的大语言模型：多语言实证研究

简介：研究者指出，大多数漏洞检测研究集中在 C/C++代码漏洞数据集，语言多样性不足，深度学习方法包括大语言模型在其他语言软件漏洞检测中的有效性尚待探索。为此，他们使用不同提示和角色策略，评估六种先进预训练大语言模型（GPT-3.5-Turbo、GPT-4 Turbo、GPT-4o、CodeLlama-7B、CodeLlama-13B 和 Gemini 1.5 Pro）在五种编程语言（Python、C、C++、Java 和 JavaScript）中检测和分类通用弱点枚举（CWE）的有效性。研究者编译多语言漏洞数据集以确保代表性，结果显示 GPT-4o 在少样本设置下漏洞检测和 CWE 分类得分最高。此外，他们还开发了与 VSCode 集成的 CODEGUARDIAN 库，通过涉及 22 位行业开发人员的用户研究表明，使用该库可使开发人员更准确快速地检测漏洞。

*链接：https://arxiv.org/abs/2408.06428*


![secgeek-foot](https://www.gptsecurity.info/img/secgeek-foot.png)
