---
layout:     post
title:      "第65期|GPTSecurity周报"
date:       2024-08-26 16:00:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - Security
    - AIGC
    - GPTSecurity周报
---


![这是一张图片](https://www.gptsecurity.info/img/in-post/0807/01.jpg)

GPTSecurity是一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练Transformer（GPT）、人工智能生成内容（AIGC）以及大语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。现为了更好地知悉近一周的贡献内容，现总结如下。

## Security Papers

#### 基于第一性原理的大语言模型多轮上下文越狱攻击

简介：大语言模型（LLMs）极大地提升了从智能对话到文本生成等众多应用的性能。然而，其固有的安全漏洞已成为日益严峻的挑战，尤其是在越狱攻击方面。攻击者能够绕过 LLMs 的安全机制，突破安全约束并产生有害输出。研究者聚焦于多轮语义越狱攻击，发现现有方法未充分考虑多轮对话在攻击策略中的作用，致使在连续交互过程中出现语义偏离。为此，研究者通过考虑多轮攻击在越狱攻击中的支持作用，为多轮攻击建立了理论基础，并在此基础上提出了一种基于上下文的情境融合黑盒越狱攻击方法，名为上下文融合攻击（CFA）。该方法包括从目标中过滤和提取关键术语，围绕这些术语构建情境场景，将目标动态地融入场景中，替换目标中的恶意关键术语，从而隐藏直接的恶意意图。通过在各种主流大语言模型和红队数据集上进行比较，研究者证明了 CFA 与其他多轮攻击策略相比具有更高的成功率、发散性和危害性，尤其在 Llama3 和 GPT-4 上展现出显著优势。

*链接：https://arxiv.org/abs/2408.04686*

#### 通过视觉提示注入对大型视觉-语言模型进行目标劫持的实证分析

简介：研究者探索了视觉提示注入（VPI），它恶意地利用大型视觉-语言模型（LVLMs）遵循绘制在输入图像上的指令的能力。研究者提出了一种新的 VPI 方法，即“通过视觉提示注入进行目标劫持”（GHVPI），它将大型视觉语言模型的执行任务从原始任务转换为攻击者指定的替代任务。定量分析表明，GPT-4V 容易受到 GHVPI 的攻击，并且显示出显著的 15.8%的攻击成功率，这是一个不可忽视的安全风险。研究者的分析还表明，成功的 GHVPI 需要大型视觉语言模型具有高字符识别能力和遵循指令的能力。

*链接：https://arxiv.org/abs/2408.03554*

#### 针对集成大语言模型的移动机器人系统的提示注入攻击研究

简介：研究者指出，将像 GPT-4o 这样的大语言模型（LLMs）集成到机器人系统中代表了具身人工智能的重大进步。这些模型能够处理多模态提示，从而生成更具上下文感知的响应。然而，这种集成存在挑战，其中主要问题之一是在机器人导航任务中使用大语言模型带来的潜在安全风险。这些任务需要精确和可靠的响应以确保安全有效的操作。多模态提示虽增强了机器人的理解能力，但也引入了可能被恶意利用的复杂性，例如旨在误导模型的对抗性输入可能导致错误或危险的导航决策。本研究中，研究者调查了在集成大语言模型的系统中提示注入对移动机器人性能的影响，并探索安全的提示策略以减轻这些风险。研究结果表明，通过实施强大的防御机制，攻击检测和系统性能总体上有大约 30.8%的显著提升，突出了防御机制在增强面向任务的安全性和可靠性方面的关键作用。

*链接：https://arxiv.org/abs/2408.03515*

#### SEAS：大语言模型的自进化对抗性安全优化

简介：研究者指出，随着大语言模型（LLMs）在能力和影响力方面不断提升，确保其安全性并防止有害输出已变得至关重要。一个有前途的方法是训练模型自动生成用于红队测试的对抗性提示。然而，大语言模型中不断演变的细微漏洞对当前对抗性方法的有效性构成了挑战，这些方法难以专门针对并探索这些模型的弱点。为了应对这些挑战，研究者引入了自进化对抗性安全（SEAS）优化框架，该框架通过利用模型自身生成的数据来增强安全性。SEAS 通过三个迭代阶段运行：初始化、攻击和对抗性优化，改进红队和目标模型以提高鲁棒性和安全性。这个框架减少了对人工测试的依赖，并显著增强了大语言模型的安全能力。研究者的贡献包括一个新颖的对抗性框架、一个全面的安全数据集，并且经过三轮迭代后，目标模型达到了与 GPT-4 相当的安全水平，同时红队模型在针对先进模型的攻击成功率（ASR）方面有显著提高。

*链接：https://arxiv.org/abs/2408.02632*

#### 使用生成式预训练 Transformer 模型进行自动化软件漏洞静态代码分析

简介：研究者评估了开源 GPT 模型在自动识别易受攻击代码语法（特别是针对C 和 C++源代码）方面的有效性。他们以 NIST SARD 数据集的 36 个源代码示例进行测试，该数据集不含表明漏洞存在与否的自然英语，且能精确量化 GPT 输出分类错误率。共评估了 5 个 GPT 模型，不同推理温度下各重复 100 次。最终发现这些模型不适合完全自动化漏洞扫描，因误报和漏报率过高。但在某些测试用例中表现出色，超越随机抽样，能识别易受攻击代码行，成功率较低。如将推理温度为 0.1 的 Llama-2-70b-chat-hf 应用于特定测试用例，召回分数和精度均为 1.0。

*链接：https://arxiv.org/abs/2408.00197*

#### 用于安全代码评估的大语言模型：一项多语言实证研究

简介：研究者指出，大多数漏洞检测研究集中在 C/C++代码漏洞数据集，语言多样性有限，深度学习方法在其他语言漏洞检测中的有效性未被充分探索。他们使用不同提示和角色策略，评估六种先进预训练大语言模型（如 GPT-3.5-Turbo、GPT-4o 等）在五种编程语言（Python、C、C++、Java、JavaScript）中检测和分类通用弱点枚举（CWE）的有效性。编译多语言漏洞数据集以确保代表性，结果显示 GPT-4o 在少样本设置下漏洞检测和 CWE 分类得分最高。此外，研究者开发了与 VSCode 集成的 CODEGUARDIAN 库，经 22 位开发人员用户研究表明，使用该库可使开发人员更准确快速地检测漏洞。

*链接：https://arxiv.org/abs/2408.06428*



![secgeek-foot](https://www.gptsecurity.info/img/secgeek-foot.png)
