---
layout:     post
title:      "第62期|GPTSecurity周报"
date:       2024-08-05 12:00:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - Security
    - AIGC
    - GPTSecurity周报
---



GPTSecurity是一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练Transformer（GPT）、人工智能生成内容（AIGC）以及大语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。现为了更好地知悉近一周的贡献内容，现总结如下。

## Security Papers

1. 生成式AI大语言模型的安全性研究综述

简介：研究者从计算机科学的角度出发，对通用人工智能大语言模型（GAI-LLMs）的AI安全研究趋势进行了深入调查。调查着重分析了LLMs作为生成性语言模型时可能遇到的危害和风险，强调了统一理论在区分研究发展和应用中的安全挑战的重要性。文章首先介绍了LLMs的工作原理，随后探讨了生成模型的基本限制和理解不足，特别是在参数规模扩大时的性能与安全权衡问题。研究者深入分析了LLMs与人类偏好一致性的挑战，并指出了文献和实施中的空白。最终，文章提出了解决LLMs AI安全问题的综合分析，并鼓励开发更安全、更一致的模型，同时展望了AI安全领域的未来研究方向。

链接：

*https://arxiv.org/abs/2407.18369*

2. PenHeal：一个用于自动化渗透测试和最佳修复的两阶段大语言模型框架

简介：研究者们开发了PenHeal，这是一个创新的两阶段大语言模型（LLM）框架，用以自主地识别和缓解网络安全漏洞。该框架由两个LLM驱动的组件构成：渗透测试模块负责在系统中检测多种潜在漏洞；修复模块则提供最优的修复策略。通过使用反事实提示（Counterfactual Prompting）和指导模块，该框架能够利用外部知识引导LLMs，有效探索多种可能的攻击路径。

实验结果显示，PenHeal不仅实现了漏洞识别与修复过程的自动化，而且在漏洞覆盖率上比基线模型提升了31%，修复策略的有效性提高了32%，并且相关成本降低了46%。这些成果凸显了LLMs在改革网络安全实践、提供创新的网络威胁防御解决方案方面的重大潜力。

链接：

*https://arxiv.org/abs/2407.17788*

3. 用于计算工作流中异常检测的大语言模型：从监督微调到上下文学习

简介：在计算工作流中进行异常检测对于确保系统的可靠性和安全性至关重要。然而，传统的基于规则的方法在检测新型异常时存在困难。本文利用大语言模型（LLMs）进行工作流异常检测，利用它们学习复杂数据模式的能力。研究了两种方法：1）监督微调（SFT），在这种方法中，预先训练好的LLMs在标记数据上进行微调，用于句子分类以识别异常；2）上下文学习（ICL），在这种方法中，包含任务描述和示例的提示引导LLMs在没有微调的情况下进行少次学习异常检测。本文评估了SFT模型的性能、效率和泛化能力，并探索了零次学习和少次学习ICL提示，并通过思维链提示增强了可解释性。在多个工作流数据集上的实验表明，LLMs在复杂执行中进行有效异常检测具有很大的潜力。

链接：

*https://arxiv.org/abs/2407.17545*

4. 利用大语言模型（LLM）实现自动化的全生命周期网络攻击构建

简介：在本文中，研究者们提出了AURORA，一个自动化的端到端网络攻击构建和模拟框架，利用大语言模型（LLMs）的能力，从网络威胁情报中提取知识并生成可执行代码。AURORA能够自动构建多阶段攻击计划，搭建基础设施，并执行攻击，大幅减少了安全专家的手动工作量。研究者还构建了一个攻击程序知识图谱，整合了跨来源的高级攻击技术知识，提高了攻击模拟的质量和多样性。与传统框架相比，AURORA在无人干预的情况下，能快速构建复杂攻击，且攻击技术覆盖率提高了40%。为了促进研究，研究者开源了20个模拟网络攻击的执行文件和基础设施数据集，为网络安全领域提供了宝贵的资源。

链接：

*https://arxiv.org/abs/2407.16928*

5. 大语言模型能自动破解GPT-4V的局限吗？

简介：研究者们介绍了AutoJailbreak，这是一种受提示优化启发的创新自动破解技术。他们利用大语言模型（LLMs）进行红队测试，以优化破解提示，并通过使用从弱到强的上下文学习提示来提升效率。研究者还开发了一种有效的搜索方法，该方法结合了早期停止技术，以减少优化时间和令牌的消耗。实验结果表明，AutoJailbreak在攻击成功率（ASR）上显著超越了传统方法，达到了超过95.3%的成绩。这项研究不仅为加强GPT-4V的安全性提供了新的视角，也强调了 LLMs 在破坏 GPT-4V 完整性方面的潜在利用。

链接：

*https://arxiv.org/abs/2407.16686*

6. MistralBSM：利用Mistral-7B进行车联网不当行为检测

简介：研究者们提出了一种新的车联网不当行为检测系统（MDS），该系统利用预训练的大语言模型（LLM）来增强安全性。在边缘云检测框架中，研究者们特别微调了先进的Mistral-7B LLM，以实现实时的边缘检测，同时云端部署的更大型LLM进行深度分析。通过在扩展的VeReMi数据集上进行的实验，Mistral-7B展现出了卓越的性能，准确率达到98%，超越了LLAMA2-7B和RoBERTa等其他LLM。此外，研究者们还探讨了窗口大小对计算成本的影响，以提高部署效率。这项研究显示，将LLM集成到MDS中能显著提升车辆不当行为的检测能力，进而加强车联网的安全防护，确保道路使用者的安全。

链接：

*https://arxiv.org/abs/2407.18462*







![secgeek-foot](https://www.gptsecurity.info/img/secgeek-foot.png)
