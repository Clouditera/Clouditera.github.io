---
layout:     post
title:      "【论文速读】|Arondight：使用自动生成的多模态越狱提示对大型视觉语言模型进行红队测试"
date:       2024-08-09 12:00:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
tags:
    - Security
    - AIGC
    - 论文速读
---


![这是一张图片](https://www.gptsecurity.info/img/in-post/0807/01.jpg)

本次分享论文：Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts

## 基本信息

**原文作者**：Yi Liu, Chengjun Cai, Xiaoli Zhang, Xingliang Yuan, Cong Wang

**作者单位**：香港城市大学、中国北京科技大学、澳大利亚墨尔本大学

**关键词**：大型视觉语言模型、红队测试、越狱攻击

**原文链接**：https://arxiv.org/pdf/2407.15050

**开源代码**：暂无

## 论文要点

**论文简介** ：研究者提出了一种专门为大型视觉语言模型（VLMs）设计的红队框架 Arondight，以解决现有红队测试方法在 VLMs 应用中的不足。Arondight 通过自动生成的多模态越狱攻击，包括视觉和文本提示，来评估 VLMs 的安全性。研究者在十种先进的 VLMs 上进行了评估，发现了显著的安全漏洞，特别是在生成有害内容和对齐多模态提示方面。研究者的评估显示，Arondight 在所有由 OpenAI 定义的禁止场景中，对 GPT-4 的攻击成功率平均达到 84.5%。

**研究目的** ：研究者旨在填补当前 VLMs 缺乏完善的红队测试框架的空白。通过设计 Arondight 框架，研究者希望解决现有红队方法在过渡到 VLMs 时遇到的视觉模态缺失和提示多样性不足的问题。研究者的目标是通过自动化的多模态越狱攻击，提高 VLMs 安全评估的全面性，促进 VLMs 在实际应用中的安全性和可靠性。

**研究贡献** ：
通过研究和实验，研究者发现以下两个关键点，这些发现对设计有效的攻击策略具有重要指导意义。

1. 有害图像的辅助作用：研究者发现，通过精心设计的有害图像可以显著增强文本攻击的效果。先前一些失败的文本攻击，在有害图像的辅助下能够重新起作用，这表明在所有被禁止的场景中，有害图像可能会导致 VLMs 的文本安全防护机制全面失效。

2. 文本多样性的提升作用：虽然现有研究表明，输入多样化的提示能够提高突破 VLMs 防御的成功率，但实现这一目标面临显著的挑战。这主要是因为在最大化生成有害内容的目标与提示多样性需求之间存在固有冲突。研究者的研究进一步证明，增加提示的多样性有助于提高攻击的有效性。
   
## 引言

目前，VLMs（如Google的Flamingo、Meta的LLaMa-2和OpenAI的GPT-4）在图像理解和生成任务中表现出色。然而，部署VLMs的一个重要问题是可能生成误导性或有害内容。为了防止生成不适当的内容，通常在部署前对VLMs进行严格的测试。现有方法主要依靠LLMs来生成测试用例（即提示），这种方法称为红队测试。红队测试作为一种主动措施，旨在识别和减轻VLMs的潜在漏洞或不足，从而在实际部署前增强其鲁棒性和可信度。

现有文献主要利用强化学习（RL）来训练红队LLM，构建多样化的红队数据集。然而，这些方法可能忽视视觉输入并缺乏多样性，导致提示覆盖率低和VLMs响应不理想。为了解决这些问题，本文提出了Arondight框架，专门针对VLMs进行红队测试，特别关注模态覆盖和多样性问题。

## 研究背景

VLMs面临着与其他机器学习模型类似的内部和外部安全威胁。由于训练数据集的广泛性，VLMs可能无意中生成偏见或有争议的内容。近年来的研究揭示了VLMs在提示注入攻击和越狱攻击方面的脆弱性。目前的努力主要集中在构建红队数据集，以便对目标VLMs进行全面的安全测试。然而，这些工作要么缺乏生成红队数据集的自动化一致性，要么无法覆盖所有禁止的安全场景。

本文在已有LLM红队测试框架的基础上，进一步为VLMs制定了通用提示模板和多样性评估指标，从而进行全面评估。

![这是一张图片](https://www.gptsecurity.info/img/in-post/0809/01.png)

## ARONDIGHT

研究者设计的 Arondight 框架旨在生成广泛且多样化的测试样本，涵盖图像和文本模态，从而全面评估目标 VLMs 的安全性。该框架包含五个关键步骤：创建对抗性越狱提示、生成有害图像和文本、构建多模态提示、选择攻击模式以及检测有害内容。通过这些步骤，研究者使得 Arondight 能够有效评估开源和黑盒 VLMs 的安全性能。

研究者设计了一种自动生成的多模态越狱攻击策略，覆盖图像和文本模态，并实现了多样性生成。通过对十种 VLMs 的广泛实验，结果表明 Arondight 在所有禁止场景中对 GPT-4 的攻击成功率达到 84.5%。

![这是一张图片](https://www.gptsecurity.info/img/in-post/0809/02.png)

## 实证研究

**实验设置** ：研究者在包括 GPT-4、Bing Chat、Google Bard 等在内的十种最新发布的 VLMs 上评估了 Arondight 的安全性能。为生成对抗性图像，研究者使用了 GPT-4 的 DALL·E 2 功能，并为每个禁止场景手动收集或通过 LLM 生成 100 个提示，生成了 10 个有害图像。每个查询执行十次，以减少随机效应并确保评估的全面性。这些实验旨在验证 Arondight 框架在各种实际应用场景中的有效性和可靠性。

![这是一张图片](https://www.gptsecurity.info/img/in-post/0809/03.png)

**评估结果** ：评估结果显示，Arondight在多个VLMs上展示了显著的攻击成功率，尤其在GPT-4上，其在所有14个禁止场景中的平均成功率达到84.5%。这一结果表明，Arondight能够有效发现和利用VLMs的安全漏洞。

![这是一张图片](https://www.gptsecurity.info/img/in-post/0809/04.png)

具体而言，Arondight在高度敏感的场景（如政治游说和经济危害）中的攻击成功率显著高于传统方法，例如在政治游说场景中的成功率高达98%。相比之下，传统的文本和多模态越狱攻击方法成功率较低，突显了Arondight的优势。

此外，Arondight在评估开源和商业VLMs（如MiniGPT-4和VisualGLM）时，也发现这些模型在多模态输入下存在显著安全风险。总体而言，评估结果证明了Arondight在VLMs安全评估中的有效性和必要性，为全面检测VLMs的安全性能提供了新的方法。

## 论文结论

本文提出了Arondight框架，这是第一个专门针对大型视觉语言模型（VLMs）的高效红队测试框架。Arondight通过自动生成的多模态越狱攻击，显著提升了VLMs的安全评估能力。

实验结果表明，Arondight在评估十种开源和商业VLMs时，展示了卓越的攻击成功率，特别是在GPT-4上达到了84.5%。这一框架不仅揭示了现有VLMs在多模态输入处理中的安全漏洞，还为开发者提供了改进模型防御策略的重要参考。Arondight的多样化测试方法和全面评估能力，为未来VLMs的安全性和可靠性研究奠定了坚实基础。

原作者：论文解读智能体

校对：小椰风





