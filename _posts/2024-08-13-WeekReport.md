---
layout:     post
title:      "第63期|GPTSecurity周报"
date:       2024-08-13 12:00:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - Security
    - AIGC
    - GPTSecurity周报
---

![这是一张图片](https://www.gptsecurity.info/img/in-post/0807/01.jpg)


GPTSecurity是一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练Transformer（GPT）、人工智能生成内容（AIGC）以及大语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。现为了更好地知悉近一周的贡献内容，现总结如下。


## Security Papers


 **通过意外标记进行大语言模型的自适应预训练数据检测**
  
简介：在本文中，研究者提出了一种自适应的预训练数据检测方法，该方法减轻了对大语言模型逐字记忆能力的依赖，有效增强了识别能力。研究者的方法能自适应定位输入的“意外标记”，若对一个标记的预测是“确定但错误”，即概率分布的香农熵低且真实标记概率低，此标记对模型就是意外的。通过意外标记的预测概率衡量“意外”，检测方法基于简单假设：模型看到已见过的数据比未见过的更不意外。此方法无需访问预训练数据语料库或像参考模型那样额外训练即可应用。在多种基准和模型的不同实验中，该方法持续优于现有方法，最大改进达 29.5%。此外，研究者还引入新基准 Dolma-Book，它基于新框架，用模型训练前后收集的书籍数据进一步评估。
链接：

*https://arxiv.org/abs/2407.21248*


**BadRobot: 在物理世界中对基于大语言模型的具身 AI 进行越狱**

简介：研究者正在深入研究基于大语言模型（LLMs）的具身人工智能（AI），这一技术能够通过传感器和执行器与物理世界互动，使AI在复杂的真实环境中学习和操作。LLMs在复杂任务的规划中起着关键作用，因此，基于LLM的具身AI逐渐成为研究的焦点。未来十年内，这类机器人预计将在家庭和工业领域广泛普及。然而，研究者发现，这些AI系统存在严重的安全隐患，可能会执行有害行为，违反阿西莫夫的机器人三定律，威胁人类安全。研究揭示了三大关键安全漏洞：通过受损的LLM实现对机器人的越狱、安全性在行动与语言空间之间的错配、以及欺骗性提示导致的危险行为。研究者呼吁增强对具身AI安全性的关注，并提出了潜在的缓解措施。
链接：

*https://arxiv.org/abs/2407.20242*


**大语言模型（LLMs）能被欺骗吗？探究LLMs中的漏洞**

简介：大语言模型（LLMs）在自然语言处理领域获得了广泛应用，但其存在的漏洞可能带来严重后果。一个典型的例子是，某些LLM在处理医疗文档时，可能在未经授权的情况下泄露患者个人数据。为了应对这些风险，研究者们深入分析了LLM的多种漏洞，涵盖了模型层面、训练阶段和推理阶段的问题。他们提出了一些缓解策略，包括“模型编辑”以修改LLM的行为，以及“Chroma Teaming”这种结合多种团队策略来增强LLM抗风险能力的方法。通过这些研究，旨在识别和理解LLM的现有漏洞，并探索未来改进的方向，以构建更安全、健壮的LLM系统。
链接：

*https://arxiv.org/abs/2407.20529*


**破坏智能体：通过故障放大损害自主的大语言模型智能体**

简介：近年来，基于大语言模型（LLMs）的自主智能体技术取得了显著进展，并广泛应用于实际场景。这些智能体不仅能够在现实世界中执行实际动作，如使用工具与环境互动，还扩展了基础LLM的功能。然而，由于这些系统能够执行关键操作，一旦受到攻击，其造成的损害可能比单一语言模型更为严重。因此，对这些系统潜在漏洞的评估变得尤为重要。

尽管已有研究探讨了LLM智能体的有害行为，研究者们从新的角度入手，提出了一种新型攻击方法——通过误导智能体执行重复或无关的动作来引发故障。研究者们通过多种攻击方法和评估手段进行综合测试，发现这些攻击在多个场景中导致了超过80%的失败率。这些实验结果突显了漏洞的现实风险，并提出了自我检查检测方法作为缓解措施。然而，研究表明，仅依靠LLM进行攻击检测存在困难，这表明这些漏洞带来了显著的风险。

链接：

*https://arxiv.org/abs/2407.20859*


**通过跨模态信息检测器防御视觉语言模型中的越狱攻击**

简介：研究者们发现，视觉语言模型（VLMs）在扩展大语言模型（LLMs）对视觉信息的理解能力方面表现出色，尤其是在视觉相关任务中取得了显著成绩。然而，最近的研究表明，这些模型容易受到越狱攻击，即恶意用户通过技术手段破坏模型的安全性，生成误导性和有害的回答。这种威胁既源于LLM的固有漏洞，也受到视觉输入带来的更大攻击范围的影响。为了提高VLMs对越狱攻击的防御能力，研究者们开发了多种防御技术，但这些方法往往需要对模型进行内部结构的修改或在推理阶段消耗大量计算资源。鉴于多模态信息的双重性质，研究者们提出了CIDER（Cross-modality Information DETector），一种即插即用的越狱检测器。CIDER利用有害查询与对抗性图像之间的跨模态相似性，能够识别恶意扰动的图像输入。这个检测器简单有效，独立于目标VLMs，且计算成本较低。大量实验结果证明了CIDER的有效性和效率，以及其在白盒和黑盒VLMs中的可迁移性。

链接：

*https://arxiv.org/abs/2407.21659*


**关于使用 ChatGPT 进行软件安全的定性研究：认知与实用性**

简介：研究者们发现，随着人工智能（AI）的进步，大语言模型（LLMs）如 ChatGPT 在执行知识密集型任务方面表现出色。鉴于软件安全工程的复杂性，研究者们探索了 ChatGPT 在软件安全任务中的潜力。为了评估 ChatGPT 作为支持软件安全的技术，研究者们采取了两种方法。首先，他们分析了在 Twitter 上分享 ChatGPT 应用于安全任务经验的人员观点，发现安全从业者认为 ChatGPT 对漏洞检测、信息检索和渗透测试等任务有一定帮助。其次，研究者们设计了实验，评估 ChatGPT 在实际环境中作为 oracle 的实用性，特别关注漏洞检测。结果表明，ChatGPT 的回应通常包含通用安全信息，可能不适合行业实际应用。为了避免数据泄露，研究者们使用了 OpenAI 数据截止日期之后编制的漏洞数据集，该数据集来自真实项目，涵盖40种漏洞类型和12种编程语言。研究结果对未来开发和评估专注于软件安全的 LLMs 提供了重要参考。

链接：
*https://arxiv.org/abs/2408.00435*