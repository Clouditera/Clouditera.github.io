---
layout:     post
title:      "第56期|GPTSecurity周报"
date:       2024-08-14 17:00:00
author:     "安全极客"
header-img: "img/post-bg-unix-linux.jpg"
catalog: true
tags:
    - Security
    - AIGC
    - GPTSecurity周报
---


![这是一张图片](https://www.gptsecurity.info/img/in-post/0807/01.jpg)

GPTSecurity是一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练Transformer（GPT）、人工智能生成内容（AIGC）以及大语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。现为了更好地知悉近一周的贡献内容，现总结如下。


## Security Papers



#### SelfDefend: LLMs能够以实用的方式自我防御免受破解
  
简介：SelfDefend是一种新型的大语言模型（LLMs）防御框架，旨在对抗越狱攻击，即绕过安全对齐措施的攻击。该框架通过建立一个影子LLM实例来保护目标LLM，同时进行基于检查点的访问控制。SelfDefend利用现有LLMs识别用户查询中潜在有害内容的能力，通过实验验证了其有效性，能够显著降低攻击成功率，同时对正常查询影响极小。此外，通过数据蒸馏方法，SelfDefend还优化了开源防御模型，使其在抵御越狱攻击方面表现优异，且延迟低，进一步增强了其在实际应用中的实用性。

链接：

*https://arxiv.org/abs/2406.05498*

#### 多智能体软件开发实验：迈向统一平台
  
简介：大语言模型正在革新软件工程，通过AI技术贯穿需求收集、架构设计、代码编写、测试和部署等环节。本研究旨在构建一个统一平台，利用多个AI代理自动将用户需求转化为结构化的软件交付物，包括用户故事、优先级排序、UML图、模块化API、单元测试和端到端测试。该平台还负责任务组织、安全合规性检查，并为非功能性需求提供设计建议。用户可按偏好控制各阶段，平台遵循欧洲标准进行安全合规性检查，并提出设计优化。研究使用GPT-3.5、GPT-4和Llama3等模型生成模块化代码，并在GitHub上公开源代码，以支持研究和实践。

链接：

*https://arxiv.org/abs/2406.05381*


#### PowerPeeler：一种精确且通用的PowerShell脚本动态去混淆方法

简介：PowerShell作为强大的自动化工具，常被网络攻击者利用进行恶意脚本编写和混淆，以逃避检测和分析。为解决这一问题，本文提出了一种名为PowerPeeler的动态去混淆方法，它通过分析抽象语法树（AST）节点来识别和跟踪脚本执行过程中的混淆部分，并动态记录执行结果，从而实现精确的去混淆。与现有工具相比，PowerPeeler在去混淆正确率上达到95%，显著领先，并能恢复最多敏感数据，同时保持高语义一致性。此外，PowerPeeler在有限时间内能有效产出最多有效去混淆结果，且具有良好的可扩展性，可作为网络安全解决方案的一部分。

链接：

*https://arxiv.org/abs/2406.04027*


#### 探索针对基于大语言模型的决策制定的后门攻击

简介：大语言模型（LLMs）在特定应用的微调阶段，因其强大的常识和推理能力，在决策任务中表现出巨大潜力。然而，这一阶段也存在显著的安全风险。本研究提出了首个针对LLM决策系统的后门攻击框架（BALD），系统地探讨了如何在微调阶段通过不同渠道引入后门攻击。研究者提出了三种攻击机制：词注入、场景操纵和知识注入，并通过实验验证了这些攻击的有效性和隐蔽性。研究还评估了LLMs在决策任务中的脆弱性，并探讨了潜在的防御措施，以保护基于LLM的决策系统。

链接：

*https://arxiv.org/abs/2405.20774*


#### 针对GPT-4o的语音破解攻击

简介：随着GPT-4o这一多模态大语言模型的推出，它通过音频、视觉和文本的交互能力，将人机交互带入了更自然的领域。然而，这同时也为潜在的语音破解攻击提供了新的机会。本研究首次系统地评估了针对GPT-4o语音模式的破解攻击，并发现GPT-4o在将文本破解提示转换为语音时具有较强的抵抗性。研究者提出了VoiceJailbreak攻击，通过模拟人类行为和虚构故事讲述来诱使GPT-4o泄露敏感信息。实验表明，VoiceJailbreak能显著提高攻击成功率，从0.033提升至0.778。研究还探讨了不同因素对攻击效果的影响，并通过高级写作技巧进一步增强了攻击效果。研究者希望这项研究能为构建更安全的多模态语言模型提供帮助。

链接：

*https://arxiv.org/abs/2405.19103*


#### LLM辅助的静态分析用于检测安全漏洞

简介：软件存在安全漏洞的风险，而用于检测这些漏洞的程序分析工具在实际应用中的效果并不理想。尽管大语言模型（LLMs）在代码生成方面表现出色，但是它们难以进行复杂的代码推理以发现安全漏洞。本研究提出了一种名为IRIS的新方法，它首次将LLMs与静态分析相结合，以实现对整个代码库的推理，从而检测安全漏洞。研究者创建了一个包含120个真实Java项目中手动验证的安全漏洞的新数据集CWE-Bench-Java。IRIS利用GPT-4模型检测出其中的69个漏洞，而最先进的静态分析工具仅检测到27个。此外，IRIS还显著降低了误报率，最高减少了80%以上。这一成果不仅提高了漏洞检测的准确性，还减少了开发者的负担。

链接：

*https://arxiv.org/abs/2405.17238*





![secgeek-foot](https://www.gptsecurity.info/img/secgeek-foot.png)
